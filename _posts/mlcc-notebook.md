---
title: mlcc笔记
date: 2018-10-23 14:43:19
tags:
---
听说谷歌出了一个机器学习的课程（早就除了，致远星战况如何），去学习了一下，也算是为硕士阶段的学习做一些准备吧。

然后这里主要是一些学习过程中的笔记（估计并没有人会看）。

<!-- more -->

* `回归`用于预测可连续的值，`分类`用于预测离散的值

> 简单来说，训练模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为`经验风险最小化`。

* `平方损失`是一种常见的损失函数，（L2损失），单一样本的损失为欧几里得距离的平方

* `均方误差`是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量

* 通常，可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已`收敛`

* 梯度矢量具有方向和大小。`梯度下降`算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。

* `超参数`是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果选择的学习速率过小，就会花费太长的学习时间；如果指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳

> `小批量随机梯度下降法（小批量 SGD）`是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

* 通过截取`离群值`（将某些值明显和其他不正常的值限制在一个合理的范围之内【我是这么理解的😅）可以改进模型的拟合情况

* 总的训练的样本 = batch size（随机选择） * 训练迭代的总次数

* `泛化`是指对于全新的数据能够有较好的效果

* 通过[配套的playground](https://developers.google.cn/machine-learning/crash-course/training-and-test-sets/playground-exercise)可以推断出通过降低学习速率可以提高训练效果。多数情况下，增加批量大小不会显著影响训练损失或测试损失，有的时候，增加批量大小会是的训练效果更好。

* 将数据分为`训练数据-测试数据`可能会在不断地调整参数的过程中造成过拟合。一种较为优秀的时间是将数据分为`训练数据-验证数据-测试数据`。通过在验证数据对训练结果进行验证，而测试数据只在最后使用。