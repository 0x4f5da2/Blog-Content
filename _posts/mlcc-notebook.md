---
title: mlcc笔记
date: 2018-10-23 14:43:19
tags:
---
听说谷歌出了一个机器学习的课程（早就出了，致远星战况如何），去学习了一下，也算是为硕士阶段的学习做一些准备吧。

然后这里主要是一些学习过程中的笔记（估计并没有人会看）。

<!-- more -->

* `回归`用于预测可连续的值，`分类`用于预测离散的值

> 简单来说，训练模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为`经验风险最小化`。

* `平方损失`是一种常见的损失函数，（L2损失），单一样本的损失为欧几里得距离的平方

* `均方误差`是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量

* 通常，可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已`收敛`

* 梯度矢量具有方向和大小。`梯度下降`算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。

* `超参数`是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果选择的学习速率过小，就会花费太长的学习时间；如果指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳

> `小批量随机梯度下降法（小批量 SGD）`是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

* 通过截取`离群值`（将某些值明显和其他不正常的值限制在一个合理的范围之内【我是这么理解的😅）可以改进模型的拟合情况

* 总的训练的样本 = batch size（随机选择） * 训练迭代的总次数

* `泛化`是指对于全新的数据能够有较好的效果

* 通过[配套的playground](https://developers.google.cn/machine-learning/crash-course/training-and-test-sets/playground-exercise)可以推断出通过降低学习速率可以提高训练效果。多数情况下，增加批量大小不会显著影响训练损失或测试损失，有的时候，增加批量大小会是的训练效果更好。

* 将数据分为`训练数据-测试数据`可能会在不断地调整参数的过程中造成过拟合。一种较为优秀的时间是将数据分为`训练数据-验证数据-测试数据`。通过在验证数据对训练结果进行验证，而测试数据只在最后使用。

* shuffle dataframe
    ```python
    from sklearn.utils import shuffle
    df = shuffle(df)
    ```

* 在对数据进行预先处理的时候，需要注意遗漏值，重复样本，不良标签和不良特征值。并在可能的时候对其进行修正。

* 正则化指的是减小模型复杂度来减少过渡拟合
    * 提前停止
    * 添加模型复杂度惩罚（正则化）

* 逻辑回归会生成一个介于0到1之间的概率值，使用`对数损失函数`

* 正确率可能会在正例和负例不均衡的情况下出现一些偏差，并不能很好的反映模型的情况

* 分类
    * 精确率 = TP / （TP + FP）；在被识别为正类别的样本中，确实为正类别的比例是多少？
    * 灵敏度，召回率 = TP / （TP + FN）；在所有正类别样本中，被正确识别为正类别的比例是多少？

> 曲线下面积因以下两个原因而比较实用：
> * 曲线下面积的尺度不变。它测量预测的排名情况，而不是测量其绝对值。
> * 曲线下面积的分类阈值不变。它测量模型预测的质量，而不考虑所选的分类阈值。
>
> 不过，这两个原因都有各自的局限性，这可能会导致曲线下面积在某些用例中不太实用：
> * `并非总是希望尺度不变`。 例如，有时我们非常需要被良好校准的概率输出，而曲线下面积无法告诉我们这一结果。
> * `并非总是希望分类阈值不变`。 在假负例与假正例的代价存在较大差异的情况下，尽量减少一种类型的分类错误可能至关重要。例如，在进行垃圾邮件检测时，您可能希望优先考虑尽量减少假正例（即使这会导致假负例大幅增加）。对于此类优化，曲线下面积并非一个实用的指标。

* 预测偏差指的是预测平均值以及数据集中标签的平均值的差

* 造成预测偏差的可能原因包括
    * 特征集不完整
    * 数据集混乱
    * 模型实现流水线中有错误
    * 训练样本有偏差
    * 正则化过强

* 可以添加校准层调整模型输出，减小预测偏差（不推荐）

* L1正则化
    * 降低权重而不是权重的平方
    * 从 L2 正则化转换到 L1 正则化之后，所有已知权重将有所减少
    * 增加 L1 正则化率一般会减小已知权重，正则化率过高的时候，模型将不能够收敛
    * 使得大多数信息缺乏的特征的权重正好为0
    * 可能会使得信息丰富的特征的其暗中正好为0
    * 可以用于减小模型的大小

